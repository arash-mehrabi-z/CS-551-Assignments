{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d0c5c6-0f67-4e7d-99f4-6de5c5bd05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ccaa23-b90b-4806-aaae-47fadbf69985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the parameters.\n",
    "RANDOM_STATE = 123\n",
    "DATASET_NAME = 'mnist_784'\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f758211-a65d-4839-a3ce-f16312e5f8ba",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c69f4be-04fa-4c31-9b9a-5343e16e4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(display_details: bool = False) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "        This method fetches the corresponding dataset and splits train and test sets. Also, it displays the details\n",
    "        of the dataset.\n",
    "        DO NOT CHANGE THE METHOD!\n",
    "    :return: X_train, X_test, Y_train, Y_test as NumPy Array.\n",
    "    \"\"\"\n",
    "    X, Y = fetch_openml(DATASET_NAME, version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, shuffle=True,\n",
    "                                                        random_state=RANDOM_STATE, stratify=Y)\n",
    "\n",
    "    if display_details:\n",
    "        print(\"Train Size:\", X_train.shape[0], \"\\tTest Size:\", X_test.shape[0])\n",
    "        print(\"Feature Size:\", X_train.shape[1])\n",
    "\n",
    "        train_unique, train_count = np.unique(Y_train, return_counts=True)\n",
    "        train_labels = {label: train_count[i] for i, label in enumerate(train_unique)}\n",
    "        test_unique, test_count = np.unique(Y_test, return_counts=True)\n",
    "        test_labels = {label: test_count[i] for i, label in enumerate(test_unique)}\n",
    "\n",
    "        all_labels = sorted(list(train_labels.keys() | test_labels.keys()))\n",
    "\n",
    "        print(\"\\t\\tClass Distribution:\")\n",
    "        for label in all_labels:\n",
    "            print(f\"Label: {label}\\tTrain Set: {train_labels.get(label, 0)}\\tTest Set: {test_labels.get(label, 0)}\")\n",
    "\n",
    "        plt.bar(train_labels.keys(), train_labels.values(), label=\"Train\")\n",
    "        plt.bar(test_labels.keys(), test_labels.values(), label=\"Test\")\n",
    "        plt.title(\"Class Distribution\")\n",
    "        plt.xticks(all_labels)\n",
    "        plt.xlabel(\"Labels\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"class_distribution.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01549ca-46c9-41a4-9dda-0adb93012201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 56000 \tTest Size: 14000\n",
      "Feature Size: 784\n",
      "\t\tClass Distribution:\n",
      "Label: 0\tTrain Set: 5522\tTest Set: 1381\n",
      "Label: 1\tTrain Set: 6302\tTest Set: 1575\n",
      "Label: 2\tTrain Set: 5592\tTest Set: 1398\n",
      "Label: 3\tTrain Set: 5713\tTest Set: 1428\n",
      "Label: 4\tTrain Set: 5459\tTest Set: 1365\n",
      "Label: 5\tTrain Set: 5050\tTest Set: 1263\n",
      "Label: 6\tTrain Set: 5501\tTest Set: 1375\n",
      "Label: 7\tTrain Set: 5834\tTest Set: 1459\n",
      "Label: 8\tTrain Set: 5460\tTest Set: 1365\n",
      "Label: 9\tTrain Set: 5567\tTest Set: 1391\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d7a507-006f-4a3f-a1e7-3921a5657d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a441bd-68b4-4c95-90ab-fcb177836b99",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510ce6b8-3805-42b7-93d6-2a6805256c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(y_true , y_pred, title):\n",
    "    conf_mat = confusion_matrix(y_true , y_pred)\n",
    "    precision, recall, fscore, support = score(y_true, y_pred, labels=all_labels)\n",
    "    accuracy = conf_mat.diagonal()/conf_mat.sum(axis=1)\n",
    "    accuracy = accuracy.round(5) * 100\n",
    "    precision = precision.round(2)\n",
    "    recall = recall.round(2)\n",
    "    fscore = fscore.round(2)\n",
    "    \n",
    "    print(\"Total Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    print('avg accuracy: {}'.format(np.sum(accuracy)/accuracy.size))\n",
    "    print('precision: {}'.format(precision))\n",
    "    print('avg precision: {}'.format(np.sum(precision) / precision.size))\n",
    "    print('recall: {}'.format(recall))\n",
    "    print('avg recall: {}'.format(np.sum(recall) / recall.size))\n",
    "    print('fscore: {}'.format(fscore))\n",
    "    print('avg fscore: {}'.format(np.sum(fscore) / fscore.size))\n",
    "    print('support: {}'.format(support))\n",
    "    \n",
    "    precision, recall, fscore, support = score(y_true, y_pred, labels=all_labels, average='weighted')\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(\"weighted Total Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print('weighted avg accuracy: {}'.format(np.sum(accuracy)/accuracy.size))\n",
    "    print('weighted avg precision: {}'.format(np.sum(precision) / precision.size))\n",
    "    print('weighted avg recall: {}'.format(np.sum(recall) / recall.size))\n",
    "    print('weighted avg fscore: {}'.format(np.sum(fscore) / fscore.size))\n",
    "    print('weighted support: {}'.format(support))\n",
    "    \n",
    "    # plt.figure(figsize=(16,9))\n",
    "    # sn.heatmap(conf_mat, annot=True, fmt='d')\n",
    "    # plt.title(title)\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.ylabel('Truth')\n",
    "    # plt.savefig(f\"results/{title}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8da736-844a-4cf0-9515-803edbb36365",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98578136-4a23-479a-aff7-b8f32c940cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03b66b-54ff-48f9-b32c-95dbbf9a28b2",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5106025-9958-461e-ba1b-4e00301c8f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " KNN with K=1 On the Training Set\n",
      "Total Accuracy: 1.0\n",
      "accuracy: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "avg accuracy: 100.0\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 1.0\n",
      "weighted avg accuracy: 100.0\n",
      "weighted avg precision: 1.0\n",
      "weighted avg recall: 1.0\n",
      "weighted avg fscore: 1.0\n",
      "weighted support: None\n",
      "\n",
      " KNN with K=1 On the Test Set\n",
      "Total Accuracy: 0.9494285714285714\n",
      "accuracy: [98.407 99.048 94.492 94.538 93.187 92.874 97.018 94.928 91.795 92.38 ]\n",
      "avg accuracy: 94.8667\n",
      "precision: [0.96 0.97 0.96 0.94 0.95 0.94 0.97 0.94 0.96 0.9 ]\n",
      "avg precision: 0.9490000000000001\n",
      "recall: [0.98 0.99 0.94 0.95 0.93 0.93 0.97 0.95 0.92 0.92]\n",
      "avg recall: 0.9480000000000001\n",
      "fscore: [0.97 0.98 0.95 0.94 0.94 0.93 0.97 0.95 0.94 0.91]\n",
      "avg fscore: 0.9480000000000001\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9494285714285714\n",
      "weighted avg accuracy: 94.8667\n",
      "weighted avg precision: 0.9494953316915328\n",
      "weighted avg recall: 0.9494285714285714\n",
      "weighted avg fscore: 0.9493634990044435\n",
      "weighted support: None\n",
      "\n",
      " KNN with K=3 On the Training Set\n",
      "Total Accuracy: 0.9712142857142857\n",
      "accuracy: [99.312 99.492 96.906 97.304 96.208 95.96  98.437 96.743 94.176 96.228]\n",
      "avg accuracy: 97.07660000000001\n",
      "precision: [0.97 0.97 0.97 0.96 0.98 0.97 0.98 0.97 0.99 0.96]\n",
      "avg precision: 0.9719999999999999\n",
      "recall: [0.99 0.99 0.97 0.97 0.96 0.96 0.98 0.97 0.94 0.96]\n",
      "avg recall: 0.9690000000000001\n",
      "fscore: [0.98 0.98 0.97 0.97 0.97 0.96 0.98 0.97 0.96 0.96]\n",
      "avg fscore: 0.97\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 0.9712142857142857\n",
      "weighted avg accuracy: 97.07660000000001\n",
      "weighted avg precision: 0.9713121025936425\n",
      "weighted avg recall: 0.9712142857142857\n",
      "weighted avg fscore: 0.9711692172198096\n",
      "weighted support: None\n",
      "\n",
      " KNN with K=3 On the Test Set\n",
      "Total Accuracy: 0.9484285714285714\n",
      "accuracy: [98.624 99.175 94.778 94.958 93.333 93.032 97.309 93.352 90.403 92.739]\n",
      "avg accuracy: 94.77029999999999\n",
      "precision: [0.95 0.96 0.95 0.94 0.95 0.94 0.97 0.95 0.96 0.91]\n",
      "avg precision: 0.9480000000000001\n",
      "recall: [0.99 0.99 0.95 0.95 0.93 0.93 0.97 0.93 0.9  0.93]\n",
      "avg recall: 0.9469999999999998\n",
      "fscore: [0.97 0.97 0.95 0.94 0.94 0.93 0.97 0.94 0.93 0.92]\n",
      "avg fscore: 0.9460000000000001\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9484285714285714\n",
      "weighted avg accuracy: 94.77029999999999\n",
      "weighted avg precision: 0.9485193225234805\n",
      "weighted avg recall: 0.9484285714285714\n",
      "weighted avg fscore: 0.9482941779275064\n",
      "weighted support: None\n",
      "\n",
      " KNN with K=7 On the Training Set\n",
      "Total Accuracy: 0.9575178571428572\n",
      "accuracy: [98.75  99.334 94.975 96.097 94.486 93.96  97.964 95.441 91.374 94.449]\n",
      "avg accuracy: 95.68300000000002\n",
      "precision: [0.97 0.96 0.96 0.94 0.97 0.95 0.96 0.95 0.98 0.93]\n",
      "avg precision: 0.9570000000000001\n",
      "recall: [0.99 0.99 0.95 0.96 0.94 0.94 0.98 0.95 0.91 0.94]\n",
      "avg recall: 0.9549999999999998\n",
      "fscore: [0.98 0.98 0.96 0.95 0.96 0.94 0.97 0.95 0.95 0.94]\n",
      "avg fscore: 0.9579999999999999\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 0.9575178571428572\n",
      "weighted avg accuracy: 95.68300000000002\n",
      "weighted avg precision: 0.957715378684039\n",
      "weighted avg recall: 0.9575178571428572\n",
      "weighted avg fscore: 0.9574209999248403\n",
      "weighted support: None\n",
      "\n",
      " KNN with K=7 On the Test Set\n",
      "Total Accuracy: 0.9469285714285715\n",
      "accuracy: [98.479 99.175 94.635 94.608 93.407 92.478 97.382 93.077 90.549 92.38 ]\n",
      "avg accuracy: 94.61699999999999\n",
      "precision: [0.95 0.96 0.95 0.94 0.95 0.93 0.96 0.95 0.97 0.91]\n",
      "avg precision: 0.9470000000000001\n",
      "recall: [0.98 0.99 0.95 0.95 0.93 0.92 0.97 0.93 0.91 0.92]\n",
      "avg recall: 0.945\n",
      "fscore: [0.97 0.98 0.95 0.94 0.94 0.93 0.97 0.94 0.94 0.92]\n",
      "avg fscore: 0.9480000000000001\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9469285714285715\n",
      "weighted avg accuracy: 94.61699999999999\n",
      "weighted avg precision: 0.9470500214470688\n",
      "weighted avg recall: 0.9469285714285715\n",
      "weighted avg fscore: 0.9468063984800642\n",
      "weighted support: None\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = [1, 3, 7]\n",
    "\n",
    "for n in n_neighbors:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_knn = knn_model.predict(X_train)\n",
    "    title = \"KNN with K=\" + str(n) + \" On the Training Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_train, y_pred_knn, title)\n",
    "    \n",
    "    y_pred_knn = knn_model.predict(X_test)\n",
    "    title = \"KNN with K=\" + str(n) + \" On the Test Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_test, y_pred_knn, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4893d3c8-332a-425a-ae10-cef6fe7bb11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " KNN with K=1 and weights=distance On the Training Set\n",
      "Total Accuracy: 1.0\n",
      "accuracy: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "avg accuracy: 100.0\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 1.0\n",
      "weighted avg accuracy: 100.0\n",
      "weighted avg precision: 1.0\n",
      "weighted avg recall: 1.0\n",
      "weighted avg fscore: 1.0\n",
      "weighted support: None\n",
      "\n",
      " KNN with K=1 and weights=distance On the Test Set\n",
      "Total Accuracy: 0.9494285714285714\n",
      "accuracy: [98.407 99.048 94.492 94.538 93.187 92.874 97.018 94.928 91.795 92.38 ]\n",
      "avg accuracy: 94.8667\n",
      "precision: [0.96 0.97 0.96 0.94 0.95 0.94 0.97 0.94 0.96 0.9 ]\n",
      "avg precision: 0.9490000000000001\n",
      "recall: [0.98 0.99 0.94 0.95 0.93 0.93 0.97 0.95 0.92 0.92]\n",
      "avg recall: 0.9480000000000001\n",
      "fscore: [0.97 0.98 0.95 0.94 0.94 0.93 0.97 0.95 0.94 0.91]\n",
      "avg fscore: 0.9480000000000001\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9494285714285714\n",
      "weighted avg accuracy: 94.8667\n",
      "weighted avg precision: 0.9494953316915328\n",
      "weighted avg recall: 0.9494285714285714\n",
      "weighted avg fscore: 0.9493634990044435\n",
      "weighted support: None\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=1, n_jobs=-1, weights='distance')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn_model.predict(X_train)\n",
    "title = \"KNN with K=1 and weights=distance\" + \" On the Training Set\"\n",
    "print('\\n', title)\n",
    "report_metrics(y_train, y_pred_knn, title)\n",
    "\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "title = \"KNN with K=1 and weights=distance\" + \" On the Test Set\"\n",
    "print('\\n', title)\n",
    "report_metrics(y_test, y_pred_knn, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa7b4e-0db2-414c-b525-5282837a5435",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68840f08-c50f-4b0b-be28-a239d26c2dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Decision Tree with min_impurity_decrease=1e-07 On the Training Set\n",
      "Total Accuracy: 1.0\n",
      "accuracy: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "avg accuracy: 100.0\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 1.0\n",
      "weighted avg accuracy: 100.0\n",
      "weighted avg precision: 1.0\n",
      "weighted avg recall: 1.0\n",
      "weighted avg fscore: 1.0\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with min_impurity_decrease=1e-07 On the Test Set\n",
      "Total Accuracy: 0.7814285714285715\n",
      "accuracy: [91.021 95.619 44.349 82.213 81.026 79.493 51.2   89.582 82.93  81.237]\n",
      "avg accuracy: 77.86699999999999\n",
      "precision: [0.78 0.91 0.74 0.7  0.83 0.76 0.84 0.9  0.6  0.81]\n",
      "avg precision: 0.7869999999999999\n",
      "recall: [0.91 0.96 0.44 0.82 0.81 0.79 0.51 0.9  0.83 0.81]\n",
      "avg recall: 0.7780000000000001\n",
      "fscore: [0.84 0.94 0.56 0.76 0.82 0.78 0.64 0.9  0.7  0.81]\n",
      "avg fscore: 0.775\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.7814285714285715\n",
      "weighted avg accuracy: 77.86699999999999\n",
      "weighted avg precision: 0.7900817070781154\n",
      "weighted avg recall: 0.7814285714285715\n",
      "weighted avg fscore: 0.7750895280162293\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with min_impurity_decrease=1e-05 On the Training Set\n",
      "Total Accuracy: 0.9977678571428571\n",
      "accuracy: [100.     99.921  99.768  99.545  99.78   99.683  99.873  99.811  99.67\n",
      "  99.695]\n",
      "avg accuracy: 99.7746\n",
      "precision: [1.   1.   1.   1.   1.   1.   1.   1.   1.   0.99]\n",
      "avg precision: 0.999\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 0.9977678571428571\n",
      "weighted avg accuracy: 99.7746\n",
      "weighted avg precision: 0.9977693347331237\n",
      "weighted avg recall: 0.9977678571428571\n",
      "weighted avg fscore: 0.997767745307879\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with min_impurity_decrease=1e-05 On the Test Set\n",
      "Total Accuracy: 0.7915\n",
      "accuracy: [90.442 95.302 47.21  83.403 81.245 80.206 54.982 89.65  83.736 82.746]\n",
      "avg accuracy: 78.8922\n",
      "precision: [0.87 0.93 0.79 0.73 0.81 0.74 0.82 0.9  0.58 0.82]\n",
      "avg precision: 0.799\n",
      "recall: [0.9  0.95 0.47 0.83 0.81 0.8  0.55 0.9  0.84 0.83]\n",
      "avg recall: 0.788\n",
      "fscore: [0.89 0.94 0.59 0.78 0.81 0.77 0.66 0.9  0.68 0.82]\n",
      "avg fscore: 0.784\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.7915\n",
      "weighted avg accuracy: 78.8922\n",
      "weighted avg precision: 0.8024748523564953\n",
      "weighted avg recall: 0.7915\n",
      "weighted avg fscore: 0.7875614889406415\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with min_impurity_decrease=0.001 On the Training Set\n",
      "Total Accuracy: 0.8002142857142858\n",
      "accuracy: [87.631 90.463 72.085 74.287 82.598 65.743 81.113 85.636 76.85  81.085]\n",
      "avg accuracy: 79.74910000000001\n",
      "precision: [0.87 0.91 0.75 0.72 0.79 0.81 0.83 0.84 0.66 0.82]\n",
      "avg precision: 0.8\n",
      "recall: [0.88 0.9  0.72 0.74 0.83 0.66 0.81 0.86 0.77 0.81]\n",
      "avg recall: 0.798\n",
      "fscore: [0.88 0.91 0.74 0.73 0.81 0.73 0.82 0.85 0.71 0.82]\n",
      "avg fscore: 0.8\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 0.8002142857142858\n",
      "weighted avg accuracy: 79.74910000000001\n",
      "weighted avg precision: 0.802886755308962\n",
      "weighted avg recall: 0.8002142857142858\n",
      "weighted avg fscore: 0.8002911141955631\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with min_impurity_decrease=0.001 On the Test Set\n",
      "Total Accuracy: 0.6865714285714286\n",
      "accuracy: [85.083 89.206 18.813 74.65  75.385 66.033 28.945 84.853 81.538 78.433]\n",
      "avg accuracy: 68.29390000000001\n",
      "precision: [0.76 0.9  0.55 0.69 0.83 0.71 0.8  0.83 0.36 0.81]\n",
      "avg precision: 0.724\n",
      "recall: [0.85 0.89 0.19 0.75 0.75 0.66 0.29 0.85 0.82 0.78]\n",
      "avg recall: 0.683\n",
      "fscore: [0.8  0.9  0.28 0.71 0.79 0.69 0.42 0.84 0.5  0.8 ]\n",
      "avg fscore: 0.673\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.6865714285714286\n",
      "weighted avg accuracy: 68.29390000000001\n",
      "weighted avg precision: 0.7255031909536148\n",
      "weighted avg recall: 0.6865714285714286\n",
      "weighted avg fscore: 0.6763754748294248\n",
      "weighted support: None\n"
     ]
    }
   ],
   "source": [
    "min_impurity_decreases = [1e-7, 1e-5, 1e-3]\n",
    "\n",
    "for m in min_impurity_decreases:\n",
    "    dtree_model = DecisionTreeClassifier(min_impurity_decrease=m)\n",
    "    dtree_model.fit(X_train , y_train)\n",
    "    \n",
    "    y_pred_dtree = dtree_model.predict(X_train)\n",
    "    title = \"Decision Tree with min_impurity_decrease=\" + str(m) + \" On the Training Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_train, y_pred_dtree, title)\n",
    "    \n",
    "    y_pred_dtree = dtree_model.predict(X_test)\n",
    "    title = \"Decision Tree with min_impurity_decrease=\" + str(m) + \" On the Test Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_test, y_pred_dtree, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469f6c7b-3dd3-418b-a160-22c14e2fd7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Decision Tree with max depth=784 On the Training Set\n",
      "Total Accuracy: 1.0\n",
      "accuracy: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "avg accuracy: 100.0\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 1.0\n",
      "weighted avg accuracy: 100.0\n",
      "weighted avg precision: 1.0\n",
      "weighted avg recall: 1.0\n",
      "weighted avg fscore: 1.0\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with max depth=784 On the Test Set\n",
      "Total Accuracy: 0.7927142857142857\n",
      "accuracy: [90.804 95.683 48.856 82.703 81.758 81.394 55.418 89.171 83.077 81.38 ]\n",
      "avg accuracy: 79.0244\n",
      "precision: [0.79 0.93 0.8  0.73 0.83 0.76 0.84 0.89 0.61 0.81]\n",
      "avg precision: 0.799\n",
      "recall: [0.91 0.96 0.49 0.83 0.82 0.81 0.55 0.89 0.83 0.81]\n",
      "avg recall: 0.79\n",
      "fscore: [0.84 0.94 0.61 0.78 0.82 0.78 0.67 0.89 0.71 0.81]\n",
      "avg fscore: 0.7849999999999999\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.7927142857142857\n",
      "weighted avg accuracy: 79.0244\n",
      "weighted avg precision: 0.8015383940951066\n",
      "weighted avg recall: 0.7927142857142857\n",
      "weighted avg fscore: 0.7880942485059377\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with max depth=78 On the Training Set\n",
      "Total Accuracy: 1.0\n",
      "accuracy: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "avg accuracy: 100.0\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 1.0\n",
      "weighted avg accuracy: 100.0\n",
      "weighted avg precision: 1.0\n",
      "weighted avg recall: 1.0\n",
      "weighted avg fscore: 1.0\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with max depth=78 On the Test Set\n",
      "Total Accuracy: 0.7855714285714286\n",
      "accuracy: [90.804 95.683 46.066 82.563 81.172 80.839 51.418 89.034 83.223 82.171]\n",
      "avg accuracy: 78.29729999999999\n",
      "precision: [0.8  0.92 0.78 0.7  0.83 0.74 0.82 0.9  0.61 0.81]\n",
      "avg precision: 0.791\n",
      "recall: [0.91 0.96 0.46 0.83 0.81 0.81 0.51 0.89 0.83 0.82]\n",
      "avg recall: 0.783\n",
      "fscore: [0.85 0.94 0.58 0.76 0.82 0.78 0.63 0.89 0.71 0.81]\n",
      "avg fscore: 0.7769999999999999\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.7855714285714286\n",
      "weighted avg accuracy: 78.29729999999999\n",
      "weighted avg precision: 0.7941157787298919\n",
      "weighted avg recall: 0.7855714285714286\n",
      "weighted avg fscore: 0.7794777379204159\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with max depth=56 On the Training Set\n",
      "Total Accuracy: 1.0\n",
      "accuracy: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "avg accuracy: 100.0\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 1.0\n",
      "weighted avg accuracy: 100.0\n",
      "weighted avg precision: 1.0\n",
      "weighted avg recall: 1.0\n",
      "weighted avg fscore: 1.0\n",
      "weighted support: None\n",
      "\n",
      " Decision Tree with max depth=56 On the Test Set\n",
      "Total Accuracy: 0.7795714285714286\n",
      "accuracy: [90.007 95.048 44.564 82.703 81.612 81.314 49.091 88.759 82.418 81.524]\n",
      "avg accuracy: 77.704\n",
      "precision: [0.79 0.92 0.76 0.72 0.81 0.74 0.82 0.9  0.59 0.81]\n",
      "avg precision: 0.7859999999999999\n",
      "recall: [0.9  0.95 0.45 0.83 0.82 0.81 0.49 0.89 0.82 0.82]\n",
      "avg recall: 0.778\n",
      "fscore: [0.84 0.94 0.56 0.77 0.81 0.77 0.62 0.89 0.69 0.81]\n",
      "avg fscore: 0.7699999999999999\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.7795714285714286\n",
      "weighted avg accuracy: 77.704\n",
      "weighted avg precision: 0.7890690027997909\n",
      "weighted avg recall: 0.7795714285714286\n",
      "weighted avg fscore: 0.7731505083339998\n",
      "weighted support: None\n"
     ]
    }
   ],
   "source": [
    "m_depths = [28*28, 28*28//10, 28*28//14]\n",
    "\n",
    "for m in m_depths:\n",
    "    dtree_model = DecisionTreeClassifier(min_impurity_decrease=1e-7, max_depth=m)\n",
    "    dtree_model.fit(X_train , y_train)\n",
    "    \n",
    "    y_pred_dtree = dtree_model.predict(X_train)\n",
    "    title = \"Decision Tree with max depth=\" + str(m) + \" On the Training Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_train, y_pred_dtree, title)\n",
    "    \n",
    "    y_pred_dtree = dtree_model.predict(X_test)\n",
    "    title = \"Decision Tree with max depth=\" + str(m) + \" On the Test Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_test, y_pred_dtree, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb049193-12a0-45eb-b305-cf2b4bba2b1f",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38196123-b2c1-4c86-b9a9-7d686dc4d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Random Forrest with number of trees=5 On the Training Set\n",
      "Total Accuracy: 0.9935178571428571\n",
      "accuracy: [99.837 99.952 99.535 99.317 99.542 99.228 99.6   99.349 98.553 98.509]\n",
      "avg accuracy: 99.3422\n",
      "precision: [0.99 1.   0.99 0.99 0.99 0.99 1.   1.   0.99 0.99]\n",
      "avg precision: 0.993\n",
      "recall: [1.   1.   1.   0.99 1.   0.99 1.   0.99 0.99 0.99]\n",
      "avg recall: 0.9950000000000001\n",
      "fscore: [1.   1.   0.99 0.99 0.99 0.99 1.   0.99 0.99 0.99]\n",
      "avg fscore: 0.993\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 0.9935178571428571\n",
      "weighted avg accuracy: 99.3422\n",
      "weighted avg precision: 0.9935230955605588\n",
      "weighted avg recall: 0.9935178571428571\n",
      "weighted avg fscore: 0.9935140494550059\n",
      "weighted support: None\n",
      "\n",
      " Random Forrest with number of trees=5 On the Test Set\n",
      "Total Accuracy: 0.9052142857142857\n",
      "accuracy: [95.8   97.905 91.416 89.006 92.088 84.798 91.855 90.336 85.788 84.759]\n",
      "avg accuracy: 90.3751\n",
      "precision: [0.92 0.96 0.86 0.85 0.88 0.88 0.96 0.94 0.9  0.91]\n",
      "avg precision: 0.906\n",
      "recall: [0.96 0.98 0.91 0.89 0.92 0.85 0.92 0.9  0.86 0.85]\n",
      "avg recall: 0.9039999999999999\n",
      "fscore: [0.94 0.97 0.89 0.87 0.9  0.87 0.94 0.92 0.88 0.88]\n",
      "avg fscore: 0.906\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9052142857142857\n",
      "weighted avg accuracy: 90.3751\n",
      "weighted avg precision: 0.9059248196160704\n",
      "weighted avg recall: 0.9052142857142857\n",
      "weighted avg fscore: 0.9050910303808392\n",
      "weighted support: None\n",
      "\n",
      " Random Forrest with number of trees=10 On the Training Set\n",
      "Total Accuracy: 0.9990357142857142\n",
      "accuracy: [100.     99.968  99.964  99.877  99.908  99.881  99.873  99.966  99.835\n",
      "  99.749]\n",
      "avg accuracy: 99.9021\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 0.9990357142857142\n",
      "weighted avg accuracy: 99.9021\n",
      "weighted avg precision: 0.9990358097822348\n",
      "weighted avg recall: 0.9990357142857142\n",
      "weighted avg fscore: 0.9990356165125007\n",
      "weighted support: None\n",
      "\n",
      " Random Forrest with number of trees=10 On the Test Set\n",
      "Total Accuracy: 0.9368571428571428\n",
      "accuracy: [98.117 98.54  94.778 91.597 93.26  92.082 95.564 91.981 90.989 89.288]\n",
      "avg accuracy: 93.61960000000002\n",
      "precision: [0.95 0.98 0.91 0.91 0.92 0.92 0.97 0.96 0.92 0.93]\n",
      "avg precision: 0.9369999999999999\n",
      "recall: [0.98 0.99 0.95 0.92 0.93 0.92 0.96 0.92 0.91 0.89]\n",
      "avg recall: 0.937\n",
      "fscore: [0.97 0.98 0.93 0.91 0.93 0.92 0.96 0.94 0.91 0.91]\n",
      "avg fscore: 0.9359999999999999\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9368571428571428\n",
      "weighted avg accuracy: 93.61960000000002\n",
      "weighted avg precision: 0.9369506590901376\n",
      "weighted avg recall: 0.9368571428571428\n",
      "weighted avg fscore: 0.9367732641090678\n",
      "weighted support: None\n",
      "\n",
      " Random Forrest with number of trees=40 On the Training Set\n",
      "Total Accuracy: 0.9999821428571428\n",
      "accuracy: [100.    100.    100.    100.    100.    100.    100.    100.    100.\n",
      "  99.982]\n",
      "avg accuracy: 99.9982\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 0.9999821428571428\n",
      "weighted avg accuracy: 99.9982\n",
      "weighted avg precision: 0.9999821460903754\n",
      "weighted avg recall: 0.9999821428571428\n",
      "weighted avg fscore: 0.9999821428635326\n",
      "weighted support: None\n",
      "\n",
      " Random Forrest with number of trees=40 On the Test Set\n",
      "Total Accuracy: 0.9613571428571429\n",
      "accuracy: [98.262 98.54  96.71  94.888 95.238 94.695 97.745 95.202 94.872 94.824]\n",
      "avg accuracy: 96.09759999999999\n",
      "precision: [0.98 0.99 0.95 0.94 0.96 0.96 0.97 0.98 0.94 0.94]\n",
      "avg precision: 0.961\n",
      "recall: [0.98 0.99 0.97 0.95 0.95 0.95 0.98 0.95 0.95 0.95]\n",
      "avg recall: 0.962\n",
      "fscore: [0.98 0.99 0.96 0.95 0.96 0.95 0.97 0.96 0.94 0.95]\n",
      "avg fscore: 0.961\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9613571428571429\n",
      "weighted avg accuracy: 96.09759999999999\n",
      "weighted avg precision: 0.9614684806925293\n",
      "weighted avg recall: 0.9613571428571429\n",
      "weighted avg fscore: 0.9613739738983583\n",
      "weighted support: None\n",
      "\n",
      " Random Forrest with number of trees=100 On the Training Set\n",
      "Total Accuracy: 1.0\n",
      "accuracy: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "avg accuracy: 100.0\n",
      "precision: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg precision: 1.0\n",
      "recall: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg recall: 1.0\n",
      "fscore: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg fscore: 1.0\n",
      "support: [5522 6302 5592 5713 5459 5050 5501 5834 5460 5567]\n",
      "\n",
      "weighted Total Accuracy: 1.0\n",
      "weighted avg accuracy: 100.0\n",
      "weighted avg precision: 1.0\n",
      "weighted avg recall: 1.0\n",
      "weighted avg fscore: 1.0\n",
      "weighted support: None\n",
      "\n",
      " Random Forrest with number of trees=100 On the Test Set\n",
      "Total Accuracy: 0.9660714285714286\n",
      "accuracy: [98.697 98.413 96.853 96.218 96.19  95.408 97.673 95.476 95.824 95.04 ]\n",
      "avg accuracy: 96.5792\n",
      "precision: [0.98 0.99 0.96 0.95 0.97 0.97 0.98 0.97 0.94 0.95]\n",
      "avg precision: 0.9659999999999999\n",
      "recall: [0.99 0.98 0.97 0.96 0.96 0.95 0.98 0.95 0.96 0.95]\n",
      "avg recall: 0.9649999999999999\n",
      "fscore: [0.98 0.99 0.96 0.96 0.96 0.96 0.98 0.96 0.95 0.95]\n",
      "avg fscore: 0.9649999999999999\n",
      "support: [1381 1575 1398 1428 1365 1263 1375 1459 1365 1391]\n",
      "\n",
      "weighted Total Accuracy: 0.9660714285714286\n",
      "weighted avg accuracy: 96.5792\n",
      "weighted avg precision: 0.9661556949291348\n",
      "weighted avg recall: 0.9660714285714286\n",
      "weighted avg fscore: 0.9660833865550217\n",
      "weighted support: None\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [5, 10, 40, 100]\n",
    "\n",
    "for n in n_estimators:\n",
    "    rforest_model = RandomForestClassifier(n_estimators=n)\n",
    "    rforest_model.fit(X_train , y_train)\n",
    "    \n",
    "    y_pred_rforest = rforest_model.predict(X_train)\n",
    "    title = \"Random Forrest with number of trees=\" + str(n) + \" On the Training Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_train, y_pred_rforest, title)\n",
    "    \n",
    "    y_pred_rforest = rforest_model.predict(X_test)\n",
    "    title = \"Random Forrest with number of trees=\" + str(n) + \" On the Test Set\"\n",
    "    print('\\n', title)\n",
    "    report_metrics(y_test, y_pred_rforest, title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0c040-1f3f-44e3-93e0-bccaeb239112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
